{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parkerallan/FutureMakers-2022-Team-Deep-Green/blob/master/GAN_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZKbyU2-AiY-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wx-zNbLqB4K8",
        "outputId": "c82268fc-200c-4e7a-e7a4-b296c1deb36c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzTlj4YdCip_",
        "outputId": "db412ab0-912f-43e2-92a6-c06d08ed3e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading next-day-wildfire-spread.zip to /content\n",
            " 99% 2.06G/2.08G [00:12<00:00, 213MB/s]\n",
            "100% 2.08G/2.08G [00:13<00:00, 171MB/s]\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download fantineh/next-day-wildfire-spread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha5v3dyHSuOy",
        "outputId": "085907f4-d7ab-43e7-e4c5-d60e6431a12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  next-day-wildfire-spread.zip\n",
            "  inflating: next_day_wildfire_spread_eval_00.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_eval_01.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_test_00.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_test_01.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_00.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_01.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_02.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_03.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_04.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_05.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_06.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_07.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_08.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_09.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_10.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_11.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_12.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_13.tfrecord  \n",
            "  inflating: next_day_wildfire_spread_train_14.tfrecord  \n"
          ]
        }
      ],
      "source": [
        "!unzip next-day-wildfire-spread.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfIk2es3hJEd"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from typing import Dict, List, Optional, Text, Tuple\n",
        "from matplotlib import colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3ifsmCOTQEL"
      },
      "outputs": [],
      "source": [
        "train_dataset = '/content/next_day_wildfire_spread_train_*'\n",
        "test_dataset = '/content/next_day_wildfire_spread_test_*'\n",
        "\n",
        "#train_dataset = train_dataset.reshape(train_dataset.shape[0], 28, 28, 1).astype('float32')\n",
        "#train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "#train_dataset = tf.reshape(inputs, [28,28,12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### The Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bpTcDqoLWjY"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(32,32,3)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    # model.add(layers.Reshape((7, 7, 256)))\n",
        "    # assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    # assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    # assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
        "    # assert model.output_shape == (None, 32, 32, 1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_generator_model().summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DlrzdFJs--6",
        "outputId": "35984d56-7190-46aa-cca9-b09f9a3f5989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 32, 32, 12544)     37632     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 32, 32, 12544)    50176     \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 12544)     0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 32, 32, 128)      40140800  \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 32, 32, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 32, 32, 64)       204800    \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 32, 32, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 32, 32, 1)        1600      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,435,776\n",
            "Trainable params: 40,410,304\n",
            "Non-trainable params: 25,472\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "###Test out the generator model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl7jcC7TdPTG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "c30669e4-e886-4d51-8317-2286f2f28892"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b0bf53597999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# plt.imshow(generated_image[0, :, :, 0], cmap='viridis')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\u001b[0m\u001b[1;32m    265\u001b[0m                              \u001b[0;34m'incompatible with the layer: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                              \u001b[0;34mf'expected shape={spec.shape}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"sequential_9\" is incompatible with the layer: expected shape=(None, 32, 32, 3), found shape=(32, 32, 3)"
          ]
        }
      ],
      "source": [
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 1024*3])\n",
        "noise = noise.reshape(32,32,3)\n",
        "\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "# plt.imshow(generated_image[0, :, :, 0], cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The Discriminator\n",
        "\n",
        "The discriminator is a CNN-based image classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw2tPLmk2pEP"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[32, 32, 12]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPneagzCaQv"
      },
      "source": [
        "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDkA05NE6QMs"
      },
      "outputs": [],
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss and optimizers\n",
        "\n",
        "Define loss functions and optimizers for both models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "outputs": [],
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "### Discriminator loss\n",
        "\n",
        "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "outputs": [],
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "### Generator loss\n",
        "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, compare the discriminators decisions on the generated images to an array of 1s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgIc7i0th_Iu"
      },
      "source": [
        "The discriminator and the generator optimizers are different since you will train two networks separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtinsGDPJlV"
      },
      "source": [
        "### Save checkpoints\n",
        "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA1w-7s2POEy"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Define the training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jylSonrqSWfi"
      },
      "source": [
        "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t5ibNo05jCB"
      },
      "outputs": [],
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aFF7Hk3XdeW"
      },
      "source": [
        "**Generate and save images**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6yy-beGXKne"
      },
      "outputs": [],
      "source": [
        "\"\"\"Constants for the data reader.\"\"\"\n",
        "\n",
        "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n",
        "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
        "\n",
        "OUTPUT_FEATURES = ['FireMask', ]\n",
        "\n",
        "# Data statistics \n",
        "# For each variable, the statistics are ordered in the form:\n",
        "# (min_clip, max_clip, mean, standard deviation)\n",
        "DATA_STATS = {\n",
        "    # Elevation in m.\n",
        "    # 0.1 percentile, 99.9 percentile\n",
        "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
        "    # Pressure\n",
        "    # 0.1 percentile, 99.9 percentile\n",
        "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
        "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n",
        "    # Precipitation in mm.\n",
        "    # Negative values do not make sense, so min is set to 0.\n",
        "    # 0., 99.9 percentile\n",
        "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
        "    # Specific humidity.\n",
        "    # Negative values do not make sense, so min is set to 0.\n",
        "    # The range of specific humidity is up to 100% so max is 1.\n",
        "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
        "    # Wind direction in degrees clockwise from north.\n",
        "    # Thus min set to 0 and max set to 360.\n",
        "    'th': (0., 360.0, 190.32976, 72.59854),\n",
        "    # Min/max temperature in Kelvin.\n",
        "    # -20 degree C, 99.9 percentile\n",
        "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
        "    # -20 degree C, 99.9 percentile\n",
        "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
        "    # Wind speed in m/s.\n",
        "    # Negative values do not make sense, given there is a wind direction.\n",
        "    # 0., 99.9 percentile\n",
        "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
        "    # NFDRS fire danger index energy release component expressed in BTU's per\n",
        "    # square foot.\n",
        "    # Negative values do not make sense. Thus min set to zero.\n",
        "    # 0., 99.9 percentile\n",
        "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
        "    # Population density\n",
        "    # min, 99.9 percentile\n",
        "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
        "    # We don't want to normalize the FireMasks.\n",
        "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
        "    'PrevFireMask': (-1., 1., 0., 1.),\n",
        "    'FireMask': (-1., 1., 0., 1.)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr9yGpMMXNzn"
      },
      "outputs": [],
      "source": [
        "\"\"\"Library of common functions used in deep learning neural networks.\n",
        "\"\"\"\n",
        "def random_crop_input_and_output_images(\n",
        "    input_img: tf.Tensor,\n",
        "    output_img: tf.Tensor,\n",
        "    sample_size: int,\n",
        "    num_in_channels: int,\n",
        "    num_out_channels: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Randomly axis-align crop input and output image tensors.\n",
        "\n",
        "  Args:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "    sample_size: side length (square) to crop to.\n",
        "    num_in_channels: number of channels in input_img.\n",
        "    num_out_channels: number of channels in output_img.\n",
        "  Returns:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "  \"\"\"\n",
        "  combined = tf.concat([input_img, output_img], axis=2)\n",
        "  combined = tf.image.random_crop(\n",
        "      combined,\n",
        "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
        "  input_img = combined[:, :, 0:num_in_channels]\n",
        "  output_img = combined[:, :, -num_out_channels:]\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def center_crop_input_and_output_images(\n",
        "    input_img: tf.Tensor,\n",
        "    output_img: tf.Tensor,\n",
        "    sample_size: int,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Center crops input and output image tensors.\n",
        "\n",
        "  Args:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "    sample_size: side length (square) to crop to.\n",
        "  Returns:\n",
        "    input_img: tensor with dimensions HWC.\n",
        "    output_img: tensor with dimensions HWC.\n",
        "  \"\"\"\n",
        "  central_fraction = sample_size / input_img.shape[0]\n",
        "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
        "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
        "  return input_img, output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWHBU1V4XRXf"
      },
      "outputs": [],
      "source": [
        "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
        "\n",
        "def _get_base_key(key: Text) -> Text:\n",
        "  \"\"\"Extracts the base key from the provided key.\n",
        "\n",
        "  Earth Engine exports TFRecords containing each data variable with its\n",
        "  corresponding variable name. In the case of time sequences, the name of the\n",
        "  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
        "  where 'variable' is the name of the variable, and n the number of elements\n",
        "  in the time sequence. Extracting the base key ensures that each step of the\n",
        "  time sequence goes through the same normalization steps.\n",
        "  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
        "  For instance, for an input key 'variable_1', this function returns 'variable'.\n",
        "  For an input key 'variable', this function simply returns 'variable'.\n",
        "\n",
        "  Args:\n",
        "    key: Input key.\n",
        "\n",
        "  Returns:\n",
        "    The corresponding base key.\n",
        "\n",
        "  Raises:\n",
        "    ValueError when `key` does not match the expected pattern.\n",
        "  \"\"\"\n",
        "  match = re.match(r'([a-zA-Z]+)', key)\n",
        "  if match:\n",
        "    return match.group(1)\n",
        "  raise ValueError(\n",
        "      'The provided key does not match the expected pattern: {}'.format(key))\n",
        "\n",
        "\n",
        "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
        "\n",
        "  Args:\n",
        "    inputs: Inputs to clip and rescale.\n",
        "    key: Key describing the inputs.\n",
        "\n",
        "  Returns:\n",
        "    Clipped and rescaled input.\n",
        "\n",
        "  Raises:\n",
        "    ValueError if there are no data statistics available for `key`.\n",
        "  \"\"\"\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
        "\n",
        "\n",
        "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
        "  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
        "\n",
        "  Args:\n",
        "    inputs: Inputs to clip and normalize.\n",
        "    key: Key describing the inputs.\n",
        "\n",
        "  Returns:\n",
        "    Clipped and normalized input.\n",
        "\n",
        "  Raises:\n",
        "    ValueError if there are no data statistics available for `key`.\n",
        "  \"\"\"\n",
        "  base_key = _get_base_key(key)\n",
        "  if base_key not in DATA_STATS:\n",
        "    raise ValueError(\n",
        "        'No data statistics available for the requested key: {}.'.format(key))\n",
        "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
        "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
        "  inputs = inputs - mean\n",
        "  return tf.math.divide_no_nan(inputs, std)\n",
        "\n",
        "def _get_features_dict(\n",
        "    sample_size: int,\n",
        "    features: List[Text],\n",
        ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
        "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
        "\n",
        "  Args:\n",
        "    sample_size: Size of the input tiles (square).\n",
        "    features: List of feature names.\n",
        "\n",
        "  Returns:\n",
        "    A features dictionary for TensorFlow IO.\n",
        "  \"\"\"\n",
        "  sample_shape = [sample_size, sample_size]\n",
        "  features = set(features)\n",
        "  columns = [\n",
        "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
        "      for _ in features\n",
        "  ]\n",
        "  return dict(zip(features, columns))\n",
        "\n",
        "\n",
        "def _parse_fn(\n",
        "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
        "    num_in_channels: int, clip_and_normalize: bool,\n",
        "    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
        ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "  \"\"\"Reads a serialized example.\n",
        "\n",
        "  Args:\n",
        "    example_proto: A TensorFlow example protobuf.\n",
        "    data_size: Size of tiles (square) as read from input files.\n",
        "    sample_size: Size the tiles (square) when input into the model.\n",
        "    num_in_channels: Number of input channels.\n",
        "    clip_and_normalize: True if the data should be clipped and normalized.\n",
        "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
        "    random_crop: True if the data should be randomly cropped.\n",
        "    center_crop: True if the data should be cropped in the center.\n",
        "\n",
        "  Returns:\n",
        "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
        "  \"\"\"\n",
        "  if (random_crop and center_crop):\n",
        "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
        "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
        "  feature_names = input_features + output_features\n",
        "  features_dict = _get_features_dict(data_size, feature_names)\n",
        "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
        "\n",
        "  if clip_and_normalize:\n",
        "    inputs_list = [\n",
        "        _clip_and_normalize(features.get(key), key) for key in input_features\n",
        "    ]\n",
        "  elif clip_and_rescale:\n",
        "    inputs_list = [\n",
        "        _clip_and_rescale(features.get(key), key) for key in input_features\n",
        "    ]\n",
        "  else:\n",
        "    inputs_list = [features.get(key) for key in input_features]\n",
        "  \n",
        "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
        "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
        "\n",
        "  outputs_list = [features.get(key) for key in output_features]\n",
        "  assert outputs_list, 'outputs_list should not be empty'\n",
        "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
        "\n",
        "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
        "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
        "                                            'but dimensions of outputs_stacked'\n",
        "                                            f' are {outputs_stacked_shape}')\n",
        "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
        "\n",
        "  if random_crop:\n",
        "    input_img, output_img = random_crop_input_and_output_images(\n",
        "        input_img, output_img, sample_size, num_in_channels, 1)\n",
        "  if center_crop:\n",
        "    input_img, output_img = center_crop_input_and_output_images(\n",
        "        input_img, output_img, sample_size)\n",
        "  return input_img, output_img\n",
        "\n",
        "\n",
        "def get_dataset(train_data: Text, data_size: int, sample_size: int,\n",
        "                batch_size: int, num_in_channels: int, compression_type: Text,\n",
        "                clip_and_normalize: bool, clip_and_rescale: bool,\n",
        "                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
        "  \"\"\"Gets the dataset from the file pattern.\n",
        "\n",
        "  Args:\n",
        "    train_data: Input file pattern.\n",
        "    data_size: Size of tiles (square) as read from input files.\n",
        "    sample_size: Size the tiles (square) when input into the model.\n",
        "    batch_size: Batch size.\n",
        "    num_in_channels: Number of input channels.\n",
        "    compression_type: Type of compression used for the input files.\n",
        "    clip_and_normalize: True if the data should be clipped and normalized, False\n",
        "      otherwise.\n",
        "    clip_and_rescale: True if the data should be clipped and rescaled, False\n",
        "      otherwise.\n",
        "    random_crop: True if the data should be randomly cropped.\n",
        "    center_crop: True if the data shoulde be cropped in the center.\n",
        "\n",
        "  Returns:\n",
        "    A TensorFlow dataset loaded from the input file pattern, with features\n",
        "    described in the constants, and with the shapes determined from the input\n",
        "    parameters to this function.\n",
        "  \"\"\"\n",
        "  if (clip_and_normalize and clip_and_rescale):\n",
        "    raise ValueError('Cannot have both normalize and rescale.')\n",
        "  dataset = tf.data.Dataset.list_files(train_data)\n",
        "  dataset = dataset.interleave(\n",
        "      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.map(\n",
        "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
        "          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
        "          clip_and_rescale, random_crop, center_crop),\n",
        "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0v9z0EIXYN4"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset(\n",
        "      train_dataset,\n",
        "      data_size=64,\n",
        "      sample_size=32,\n",
        "      batch_size=100,\n",
        "      num_in_channels=12,\n",
        "      compression_type=None,\n",
        "      clip_and_normalize=False,\n",
        "      clip_and_rescale=False,\n",
        "      random_crop=True,\n",
        "      center_crop=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG4Bn1QVXbS1"
      },
      "outputs": [],
      "source": [
        "inputs, labels = next(iter(dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z3OEcucXeF-"
      },
      "outputs": [],
      "source": [
        "TITLES = [\n",
        "  'Elevation',\n",
        "  'Wind\\ndirection',\n",
        "  'Wind\\nvelocity',\n",
        "  'Min\\ntemp',\n",
        "  'Max\\ntemp',\n",
        "  'Humidity',\n",
        "  'Precip',\n",
        "  'Drought',\n",
        "  'Vegetation',\n",
        "  'Population\\ndensity',\n",
        "  'Energy\\nrelease\\ncomponent',\n",
        "  'Previous\\nfire\\nmask',\n",
        "  'Fire\\nmask'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4F7Vq0ZjXg3R"
      },
      "outputs": [],
      "source": [
        "# Number of rows of data samples to plot\n",
        "n_rows = 5 \n",
        "# Number of data variables\n",
        "n_features = inputs.shape[3]\n",
        "# Variables for controllong the color map for the fire masks\n",
        "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
        "BOUNDS = [-1, -0.1, 0.001, 1]\n",
        "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOKAEc6JXjks"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15,6.5))\n",
        "n_rows1 = n_rows\n",
        "n_rows = 5\n",
        "for i in range(n_rows):\n",
        "  for j in range(n_features + 1):\n",
        "    plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
        "    if i == 0:\n",
        "      plt.title(TITLES[j], fontsize=13)\n",
        "    if j < n_features - 1:\n",
        "      plt.imshow(inputs[i, :, :, j], cmap='viridis')\n",
        "    if j == n_features - 1:\n",
        "      plt.imshow(inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
        "      print(inputs[i,:,:,-1].shape)\n",
        "    if j == n_features:\n",
        "      plt.imshow(labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
        "    # plt.axis('off')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "## Train the model\n",
        "Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
        "\n",
        "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly3UN0SLLY2l"
      },
      "outputs": [],
      "source": [
        "train(inputs, EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfM4YcPVPkNO"
      },
      "source": [
        "Restore the latest checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhXsd0srPo8c"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GAN working.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}